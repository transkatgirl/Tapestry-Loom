# Tapestry Loom

A power user focused interface for LLM base models, inspired by the designs of [loom](https://github.com/socketteer/loom), [loomsidian](https://github.com/cosmicoptima/loom), [exoloom](https://exoloom.io), [logitloom](https://github.com/vgel/logitloom), and [wool](https://github.com/lyramakesmusic/wool).

> [!WARNING]
> This is beta software. Most of it works, but there are plenty of undiscovered bugs and things will randomly break from time to time. Make backups.

## Known issues

- Some documents may cause the text editor to render token boundaries incorrectly
	- This is due to a bug in egui regarding textedit underline rendering
- Tab bars are not read by screen readers
	- This is due to a bug in egui_tiles

If you are experiencing an issue not listed here or in this [repository's active issues](https://github.com/transkatgirl/Tapestry-Loom/issues), please file an issue so that it can be fixed.

## Getting started

### Binary releases

Compiled binaries can be found on the [releases page](https://github.com/transkatgirl/Tapestry-Loom/releases).

#### MacOS-specific instructions

Before using the app, you will need to run the following CLI command in the extracted folder:

```bash
xattr -d com.apple.quarantine tapestry*
```

### Compiling from source

Requires the [Rust Programming Language](https://rust-lang.org/tools/install/) and a working C compiler to be installed.

```bash
git clone --recurse-submodules https://github.com/transkatgirl/Tapestry-Loom.git
cd Tapestry-Loom
cargo build --release
```

The compiled binary can be found in the ./target/release/ folder.

#### Updating

Run the following commands in the repository folder:

```bash
git pull
git submodule update --init --recursive
cargo build --release
```

## Usage

See [Getting Started](./Getting%20Started.md) for more information on how to use the application.

The rest of this README covers the usage of external tools which Tapestry Loom can interface with.

### Local inference

[llama.cpp](https://github.com/ggml-org/llama.cpp)'s llama-server is recommended, as it has been confirmed to work properly with *all* of the features within Tapestry Loom.

**Ollama should *not* be used** due to [bad sampling settings](https://docs.ollama.com/modelfile#valid-parameters-and-values) which [cannot be overridden in API requests](https://github.com/ollama/ollama/issues/11325), along with a lack of available base models.

KoboldCpp is not recommended due to a lack of request queuing and a poor implementation of logprobs (the number of requested logprobs is entirely ignored).

The recommended CLI arguments for [llama-server](https://github.com/ggml-org/llama.cpp/tree/master/tools/server) are listed below:

```bash
llama-server --models-dir $MODEL_DIRECTORY --models-max 1 --jinja --chat-template "message.content" --ctx-size 4096 --temp 1 --top-k 0 --top-p 1 --min-p 0
```

Where `$MODEL_DIRECTORY` is set to the directory where model gguf files are stored.

(Regarding quantization: Benchmarks of how chat models are affected by quantization likely do not generalize to how base models are used. Quantization should be kept as low as reasonably possible, but `q8_0` is likely good enough for most use cases.)

Explanation of arguments:
- Only one model loaded into VRAM at a time; old models are automatically unloaded to make room for new ones
- The specified chat template passes user input directly to the model without further changes.
- Reducing the maximum context length helps reduce VRAM usage without sacrificing quality.
- The default sampling parameters (those specified by the CLI arguments) should leave the model's output distribution unchanged. **Sampling parameter defaults for chat models do not generalize to how base models are used.**
	- The sampling parameters specified in the CLI arguments will be overridden by any sampling parameters that are specified in a request.

If you are running llama-server on the same device as Tapestry Loom (and you are using the default port), you do not need to explicitly specify an endpoint URL when filling out the "OpenAI-style Completions" and "OpenAI-style ChatCompletions" templates.

#### Recommended models

If you are new to working with LLM base models, [Trinity-Mini-Base-Pre-Anneal](https://huggingface.co/mradermacher/Trinity-Mini-Base-Pre-Anneal-GGUF) or ([Trinity-Nano-Base-Pre-Anneal](https://huggingface.co/mradermacher/Trinity-Nano-Base-Pre-Anneal-GGUF) if you have <32GB of VRAM) is a good first model to try.

### Tokenization server (optional)

See [tapestry-tokenize](./tapestry-tokenize/README.md) for more information on how to configure and use the (optional) tokenization server.

Once a tokenization endpoint is configured for a model, enabling the setting "(Opportunistically) reuse output token IDs" can improve output quality, especially in weaves where nodes are being generated by one model rather than an ensemble of models.

This setting requires the inference backend to support returning token IDs (to check if this is working, hover over generated tokens in the text editor to see if they contain a token identifier). This is a non-standard addition to the OpenAI Completions API which is currently supported by very few inference backends ([llama.cpp](https://github.com/ggml-org/llama.cpp) has been confirmed to work properly with this feature).

If your inference backend returns token IDs in OpenAI-style Completions responses but they do not appear in your weaves, please file an issue.

### Migrating weaves from other Loom implementations

See [migration-assistant](./migration-assistant/README.md) for more information on how to migrate weaves from other Loom implementations to Tapestry Loom.

## Plans

- [x] Migrate to a desktop app rather than an Obsidian plugin
	- [x] Implement conversion from the old Weave format to the new one
	- [x] Implement all functionality supported by the original Obsidian plugin
- [x] Full UI redesign
	- [x] Resizable, dragable, scrollable, and collapsible settings
	- [x] Three user-switchable UIs:
		- [x] Interactive canvas + textbox, similar to [wool](https://github.com/lyramakesmusic/wool)
			- [x] Implement dragable and scrollable canvas
			- [x] Implement draggable canvas nodes
				- [x] Implement canvas node left click
				- [x] Implement canvas node right click
			- [x] Implement canvas actions
			- [x] Implement resizable & dragable textbox
			- [x] Automatically adjust canvas position & highlighting based on textbox cursor location
			- [x] Automatically scroll textbox based on canvas cursor
			- [x] Scroll newly generated nodes into view
		- [x] Compact tree + compact treelist + textbox, similar to [loomsidian](https://github.com/cosmicoptima/loom) & old Tapestry Loom
			- [x] Implement resizable, dragable & scrollable tree
			- [x] Implement tree nodes
				- [x] Implement tree node content display on hover
				- [x] Implement tree node left click
				- [x] Implement tree node right click
			- [x] Implement resizable & scrollable treelist
			- [x] Implement resizable and scrollable textbox
			- [x] Automatically adjust tree position & highlighting based on textbox cursor location
			- [x] Automatically scroll textbox based on tree cursor location
			- [x] Scroll newly generated nodes into view
		- [x] Compact tree + node child list + textbox, similar to [exoloom](https://exoloom.io)
			- [x] Implement resizable, dragable & scrollable tree
			- [x] Implement tree nodes
				- [x] Implement tree node content display on hover
				- [x] Implement tree node left click
				- [x] Implement tree node right click
			- [x] Implement resizable and scrollable node child list
			- [x] Implement resizable and scrollable textbox
			- [x] Automatically adjust tree position & highlighting based on textbox cursor location
			- [x] Automatically scroll textbox based on tree cursor location
			- [x] Scroll newly generated nodes into view
	- [x] Weave metadata tab
	- [x] Better UI error handling
- [x] Keyboard shortcut implementation
	- [x] Automatically adapt keyboards shortcuts based on OS (such as Mac vs Windows/Linux)
	- [x] Repeat keypresses when a keyboard be cut is held down
- [x] Store model output token IDs in nodes and reuse them when applicable
- [x] Tapestry Loom tokenization server
- [x] Allow saving and switching between multiple inference presets
	- [x] Add keyboard shortcuts for inference presets
- [x] Add ability to override model colors
- [ ] Further canvas view improvements
	- [ ] Add node collapsing
	- [ ] Add node creation button
- [ ] Support for migrating weaves from other Loom implementations
	- [ ] [loomsidian](https://github.com/cosmicoptima/loom)
	- [ ] [exoloom](https://exoloom.io)
	- [ ] [loom](https://github.com/socketteer/loom)
	- [x] Package migration assistant in releases
- [ ] Better documentation & onboarding
	- [ ] Finish manual
	- [x] Binary releases

### Plans for next major version

<!--
- [ ] Server-client, multi-user WebUI
	- [ ] Support collaborating on Weaves
	- [ ] User authentication
	- [ ] User permissions
	- [ ] User rate limiting
- [ ] Event-based server-client communication to reduce bandwidth usage
- [ ] Automatic color palette generation in settings
- [ ] HTTPS support
- [ ] Compression support
	- [ ] Brotli compression for static assets
	- [ ] LZ4 compression for websocket data
-->

- [ ] Support for DAG-based Weaves, similar to this [unreleased loom implementation](https://www.youtube.com/watch?v=xDPKR271jas&list=PLFoZLLI8ZnHCaSyopkws_9344avJQ_VEQ&index=19)
	- [ ] FIM completions
		- [ ] Selected text is used to determine FIM location
	- [ ] Node copying & moving
	- [ ] Perform heavy testing of data structures and/or formal verification to prevent bugs that could result in data loss
	- [ ] Implement node "editing" UI (not actually editing node content, but editing the tree by adding nodes / splitting nodes / merging nodes), similar to [inkstream](https://inkstream.ai)
- [ ] Embedding model support
	- [ ] Node ordering by [seriation](https://www.lesswrong.com/posts/u2ww8yKp9xAB6qzcr/if-you-re-not-sure-how-to-sort-a-list-or-grid-seriate-it)
- [ ] Add sub-title of "Humanity's Last Loom"
- [ ] Further UI improvements
	- [ ] Better handle valid UTF-8 character split across multiple nodes
	- [ ] Improve graph/canvas layout algorithm
	- [ ] Improve clarity of error messages
	- [ ] Better file manager
	- [ ] Support keyboard shortcuts for all aspects of the UI, not just the weave editor
		- [ ] Aim to support navigating the entirety of the UI without a mouse
	- [ ] Improve built-in color schemes
	- [ ] Node finding
	- [ ] Customizable node sorting
		- [ ] Time added
		- [ ] Alphabetical
		- [ ] Semantic sort
	- [ ] Node bulk selection
	- [ ] Node custom ordering via drag and drop
		- [ ] Support reordering nodes in canvas and graph views as well
	- [ ] Keyboard shortcut presets
		- [ ] Built-in presets
			- [ ] [loomsidian](https://github.com/cosmicoptima/loom)-like
			- [ ] [exoloom](https://exoloom.io)-like
			- [ ] Tapestry Loom
		- [ ] Saving & loading custom presets
			- [ ] Importing & exporting custom presets
	- [ ] Support touchscreen-only devices
	- [ ] Show hovered child of active node in editor, similar to [exoloom](https://exoloom.io)
	- [ ] Add ability to add custom labels to bookmarks/nodes
	- [ ] Add ability to add custom attributes to nodes, rather than just bookmarks
- [ ] Add support for more weave migrations
	- [ ] [wool](https://github.com/lyramakesmusic/wool)
- [ ] Include documentation within app
	- [ ] Include tokenization server within app?
- [ ] Weave statistical analysis tools
- [ ] Optimize for performance whenever possible
	- [ ] Aim to have acceptable performance on weaves with ~1 million nodes, ~200k active and ~10MB of active text on low-end hardware (such as a Raspberry Pi)
		- [ ] Implement a special "link" node to allow splitting giant weaves into multiple documents
	- [ ] Optimize memory usage to be as low as reasonably possible
- [ ] Collaborative weave editing over LAN
- [ ] Adaptive looming using token entropy or [confidence](https://arxiv.org/pdf/2508.15260)
- [ ] Token streaming and display of nodes being generated
- [ ] Prefix-based duplication
- [ ] Undo/redo functionality
- [ ] Blind comparison modes
	- [ ] (Hide) Models & token probabilities / boundaries
	- [ ] (Hide) Generated node text (only showing metadata & probabilities)
- [ ] Allow adjusting proportion of completions from each model
	- [ ] Allow dynamically adjusting proportions based on usage
		- [ ] Flatten proportion bias when increasing number of completions, do the inverse when reducing completion count
- [ ] Support alternate input devices
	- [ ] Talon Voice
	- [ ] Controllers / Gamepads
	- [ ] USB DDR Pads
- [ ] Document & selection analysis tools
	- [ ] Predictability analysis using logprobs
	- [ ] Statistical analysis of various metrics (model usage, text length, logprobs, number of branches, etc)
	- [ ] Weave metadata
- [ ] Implement context window wrapping
- [ ] Support for [Standard Completions](https://standardcompletions.org) (after the specification is finalized)
- [ ] Tooling for autolooms (looms where node choices are picked by a user-provided algorithm)
- [ ] Add some sort of plugin API for building on top of Tapestry Loom???
- [ ] Implement an optional inference server using llama.cpp

See also: the [original rewrite plans](https://github.com/transkatgirl/Tapestry-Loom/blob/c8ccca0079ae186fcc7a70b955b2d2b123082d63/README.md)

Note: Tapestry Loom will be *entirely* focused on base and/or embedding models for the foreseeable future.

There are already good chat looms (such as [miniloom](https://github.com/JD-P/miniloom)) and base model looms which heavily integrate assistant functionality (such as [helm](https://github.com/Shoalstone/helm)); Tapestry Loom will **not** be one of them.